{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Numpy Dataset from TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lists of file paths and corresponding prefixes\n",
    "import os\n",
    "\n",
    "sequence_dir = \"/Volumes/JOE_LOUIS/peaks.bedgraph/bed_cleaned/sequences\"\n",
    "\n",
    "dataset_files = []\n",
    "# List all files in directory\n",
    "for file_name in os.listdir(sequence_dir):\n",
    "    \n",
    "    # helping to store file paths and prefix into final list\n",
    "    temp = []\n",
    "\n",
    "    # Full Path to file\n",
    "    file_path = os.path.join(sequence_dir,file_name)\n",
    "    \n",
    "    # Generating the prefix name\n",
    "    prefix_name = file_name.split('.')[1]\n",
    "\n",
    "    # storing file paths and prefixes into dataset files\n",
    "    temp.extend([file_path,prefix_name])\n",
    "    dataset_files.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 74 out of 74 files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# creating a list of dataframes containing sequence information\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_files = pd.DataFrame(dataset_files,columns=['file_path','file_prefix'])\n",
    "datasets_out = []\n",
    "\n",
    "for i in dataset_files.index: # use for loop when running through all files\n",
    "    \n",
    "    # store file name\n",
    "    file_path = dataset_files.loc[i,\"file_path\"]\n",
    "    file_name = dataset_files.loc[i,\"file_prefix\"]\n",
    "    \n",
    "    # updating the user\n",
    "    print(f\"Processing {i} out of {dataset_files.index[-1]} files...\",end=\"\\r\")\n",
    "    \n",
    "    # Storing sequence and relevant information\n",
    "    sequence_info = []\n",
    "    sequence_labels = []\n",
    "\n",
    "    # read file \n",
    "    with open(file_path,'r') as file:\n",
    "        \n",
    "        # Extracting usful information regarding the file\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('>'):\n",
    "                line = line.rstrip().lstrip('>')\n",
    "                line = line.split(':')\n",
    "                line = [item for i in line for item in i.split('-')]\n",
    "                \n",
    "                if line[0] == \"chrX\":\n",
    "                    x_status = True\n",
    "                else:\n",
    "                    x_status = False\n",
    "                \n",
    "                line.append(x_status)\n",
    "                line.insert(0,file_name)\n",
    "\n",
    "                sequence_labels.append(line)\n",
    "            else:\n",
    "                line = line.rstrip().upper()\n",
    "                seq_len = len(line)\n",
    "                sequence_info.append([seq_len,line])\n",
    "        \n",
    "        # organizing file information\n",
    "        sequence_labels = pd.DataFrame(sequence_labels)\n",
    "        sequence_info = pd.DataFrame(sequence_info)\n",
    "        sequence_out = pd.concat([sequence_labels,sequence_info],axis=1,ignore_index=True)\n",
    "        sequence_out.columns = ['name','chr','start','end','Xchr','seq_len','seq']\n",
    "        \n",
    "        # appending into three dimension numpy file\n",
    "        datasets_out.append(sequence_out)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the processed datasets\n",
    "import pickle\n",
    "\n",
    "with open('processed_datasets.pkl','wb') as f:\n",
    "    pickle.dump(datasets_out,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the process datasets\n",
    "import pickle\n",
    "\n",
    "with open('processed_datasets.pkl','rb') as f:\n",
    "    processed_datasets = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Xchr</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>5474</td>\n",
       "      <td>6924</td>\n",
       "      <td>False</td>\n",
       "      <td>1450</td>\n",
       "      <td>TAAGCTCGAACATAGAACATAGGCTTGAACATATAATGACTGCCTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>7024</td>\n",
       "      <td>7349</td>\n",
       "      <td>False</td>\n",
       "      <td>325</td>\n",
       "      <td>ACCTATTTGCGCATATGCGTTTATTTTTGGGATTTAATTTTAACAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>7399</td>\n",
       "      <td>10074</td>\n",
       "      <td>False</td>\n",
       "      <td>2675</td>\n",
       "      <td>TGTAGGTGATTTTATTTATTAGAATACGAATTCTTTATCTGAATCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>10124</td>\n",
       "      <td>11599</td>\n",
       "      <td>False</td>\n",
       "      <td>1475</td>\n",
       "      <td>ACTTGAAGGCTCATTAACTTACTTCTCATATTGACATATTTTCTTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>11699</td>\n",
       "      <td>16099</td>\n",
       "      <td>False</td>\n",
       "      <td>4400</td>\n",
       "      <td>AAAACATTTAAATGTAGATACGTACAAAACAGCAAATTAAAATAGG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name    chr  start    end   Xchr  \\\n",
       "0  Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2  chr2L   5474   6924  False   \n",
       "1  Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2  chr2L   7024   7349  False   \n",
       "2  Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2  chr2L   7399  10074  False   \n",
       "3  Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2  chr2L  10124  11599  False   \n",
       "4  Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2  chr2L  11699  16099  False   \n",
       "\n",
       "   seq_len                                                seq  \n",
       "0     1450  TAAGCTCGAACATAGAACATAGGCTTGAACATATAATGACTGCCTT...  \n",
       "1      325  ACCTATTTGCGCATATGCGTTTATTTTTGGGATTTAATTTTAACAT...  \n",
       "2     2675  TGTAGGTGATTTTATTTATTAGAATACGAATTCTTTATCTGAATCG...  \n",
       "3     1475  ACTTGAAGGCTCATTAACTTACTTCTCATATTGACATATTTTCTTC...  \n",
       "4     4400  AAAACATTTAAATGTAGATACGTACAAAACAGCAAATTAAAATAGG...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Sample 71 \"GSM892323_L3mbt\"... 372 of 372 sequences.\n",
      "Processing Sample 72 \"GSM948712_e2-4hr_FAIRE_peaks\"... 12533 of 12533 sequences.\n",
      "Processing Sample 73 \"GSM948713_e6-8hr_FAIRE_peaks\"... 10665 of 10665 sequences.\n",
      "Processing Sample 74 \"GSM948714_e16-18hr_FAIRE_peaks\"... 13216 of 13216 sequences.\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the DNA sequence\n",
    "import textwrap\n",
    "import numpy as np\n",
    "\n",
    "# You must process datasets 20 at a time, then merge the lists afterwards.\n",
    "for index,sequence_out in enumerate(processed_datasets[71:]):\n",
    "    \n",
    "    # clearing space: memory issue \n",
    "    seqs_processed = []\n",
    "    seq_labels_out = []\n",
    "    \n",
    "    # screening for sequences 1000bp and under\n",
    "    seqs = sequence_out[sequence_out['seq_len'] <= 1000]['seq']\n",
    "    seqs = seqs.reset_index(drop=True)\n",
    "\n",
    "    # saving the seq labels\n",
    "    labels = sequence_out[sequence_out['seq_len'] <= 1000]['Xchr']\n",
    "    labels = list(labels.reset_index(drop=True))\n",
    "    seq_labels_out.extend(labels)\n",
    "\n",
    "    # saving classification id\n",
    "    seq_id = sequence_out['name'][0]\n",
    "\n",
    "    # TOKEN SIZE!!!\n",
    "    token_size = 3\n",
    "\n",
    "    for i in seqs.index:\n",
    "        print(f'Processing Sample {index+71} \"{seq_id}\"... {i} of {seqs.index[-1]} sequences.',end='\\r')\n",
    "\n",
    "        # creating and applying pad\n",
    "        seq_len = len(seqs[i])\n",
    "        pad_length = 1000 - seq_len\n",
    "        pad = 'N' * pad_length\n",
    "        seq_temp = seqs[i] + pad\n",
    "\n",
    "        # tokenizing\n",
    "        # seq_list = textwrap.wrap(seq_temp,token_size)\n",
    "        seq_list = []\n",
    "        for i in range(len(seq_temp) - 2):\n",
    "            seq_list.append(seq_temp[i:i+token_size])\n",
    "        \n",
    "        if len(seq_list[-1]) != token_size:\n",
    "            seq_list = seq_list[:-1]\n",
    "\n",
    "        # adding classification id & finish id\n",
    "        seq_list.insert(0,seq_id)\n",
    "        seq_list.append(\"<END>\")\n",
    "\n",
    "        # append to final list\n",
    "        seqs_processed.append(seq_list)\n",
    "    \n",
    "    # exporting numpy files\n",
    "    seqs_processed = np.array(seqs_processed)\n",
    "    seq_labels_out = np.array(seq_labels_out) \n",
    "    np.save(f'X.{index+71}.{seq_id}.npy',seqs_processed)\n",
    "    np.save(f'y.{index+71}.{seq_id}.npy',seq_labels_out)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Run of Processed data with simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACT', 'GSE37864_mle', 'CAN', 'GTA', 'AAN', 'CGC', 'ACC', 'CCA', 'NNN', 'NAG', 'GTG', 'CGN', 'GGA', 'ATN', 'ATG', 'GCG', 'GCN', 'TGA', 'GTN', 'TAG', 'AGC', 'CGG', 'GCC', 'GTT', 'GSE37864_msl3', 'NCT', 'Adelman_Dmel_S2_H3K4me1_ChIP-seq_rep1and2', 'ATA', 'CTN', 'TGG', 'TCC', 'NCA', 'TGC', 'GGG', 'GNN', 'CAG', 'GSE101554_S2_dRing_ChIP_peaks', 'NNG', 'GSE99004_macs_1WTgA', 'NGA', 'CAC', 'TCN', 'GTC', 'TTA', 'GSE37864_mof', 'NAA', 'NCN', 'GAA', 'GCA', 'ACA', 'CTC', 'Adelman_Dmel_S2_RNAPolII_ChIP-seq_control_rep1and2_normalized', 'GSE101554_S2_HA_ChIP', 'NGG', 'NNA', 'Adelman_Dmel_S2_H3K4me3_ChIP-seq_rep1and2', 'GAC', 'NCG', 'NTC', 'CTT', 'AAC', 'GSE101554_S2_Rpb3_ChIP_peaks', 'CNN', 'NNT', 'GAT', 'TCT', 'NGC', 'ATC', 'AGG', 'GSE37864_msl1', 'CTA', 'TCG', 'TNN', 'AGN', 'CCG', 'GAN', 'GSE101365_idr-macs2-mock_ago2_mueller', 'ATT', 'TTC', 'CCN', 'TAA', 'AGT', 'CGA', 'NAC', 'GSE99004_macs_1dH1gA', '<END>', 'Adelman_Dmel_S2_NELF-B_ChIP-seq_rep1and2', 'ACG', 'NGT', 'GGC', 'GGN', 'GSE101554_S2_M1BP_ChIP_peaks', 'AAA', 'TCA', 'TTN', 'CCC', 'TAN', 'GSE101554_S2_GAF_ChIP_peaks', 'GSE37864_msl2', 'TAC', 'CAT', 'ACN', 'GSE85191_Adelman_Dmel_S2_Cohesin_ChIP-seq_rep1and2', 'CGT', 'AGA', 'GSE101554_S2_Ez_ChIP_peaks', 'Adelman_Dmel_S2_H3K27ac_ChIP-seq_rep1and2', 'GCT', 'CTG', 'TTT', 'NAT', 'AAG', 'NTA', 'NCC', 'TAT', 'AAT', 'CAA', 'TGT', 'TGN', 'TTG', 'GAG', 'ANN', 'GGT', 'NTT', 'NNC', 'CCT', 'NTN', 'NAN', 'NTG'}\n"
     ]
    }
   ],
   "source": [
    "# obtaining keys to use in dictionary =\n",
    "import numpy as np\n",
    "\n",
    "def get_unique_items(nested_list):\n",
    "    unique_items = set()\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            unique_items.update(get_unique_items(item))\n",
    "        else:\n",
    "            unique_items.add(item)\n",
    "    return unique_items\n",
    "\n",
    "all_keys = get_unique_items(seqs_processed)\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Items in Dictionary: 129\n"
     ]
    }
   ],
   "source": [
    "# constructing dictionary\n",
    "dna_dict = {}\n",
    "counter = 0 \n",
    "for i in all_keys:\n",
    "    dna_dict[i] = counter\n",
    "    counter+=1\n",
    "\n",
    "print(\"Number of Items in Dictionary:\",len(dna_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing keys with values in the dataset\n",
    "print(\"Original Sequence:\",seqs_processed[0][:5])\n",
    "\n",
    "def replace_values(nested_list, dictionary):\n",
    "    \"\"\"\n",
    "    Given a nested list, replaces every item with the corresponding value in a dictionary.\n",
    "    \"\"\"\n",
    "    if isinstance(nested_list, list):\n",
    "        return [replace_values(item, dictionary) for item in nested_list]\n",
    "    elif nested_list in dictionary:\n",
    "        return dictionary[nested_list]\n",
    "    else:\n",
    "        return nested_list\n",
    "\n",
    "seqs_out = replace_values(seqs_processed,dna_dict)\n",
    "print(\"Updated Sequence:\",seqs_out[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69211, 1000)\n",
      "(69211,)\n"
     ]
    }
   ],
   "source": [
    "# check on the labels\n",
    "print(np.array(seqs_out).shape)\n",
    "print(np.array(seq_labels_out).shape)\n",
    "\n",
    "seqs_out = np.array(seqs_out)\n",
    "seq_labels_out = np.array(seq_labels_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving sample numpy arrays\n",
    "np.save('sample.X.npy',seqs_out)\n",
    "np.save('sample.y.npy',seq_labels_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Acc: 0.739\n"
     ]
    }
   ],
   "source": [
    "baseline = seq_labels_out[seq_labels_out == False].shape[0] / seq_labels_out.shape[0]\n",
    "baseline = \"{:.3f}\".format(baseline)\n",
    "print(\"Baseline Acc:\",baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.6491 - acc: 0.6972 - val_loss: 0.5482 - val_acc: 0.7369\n",
      "Epoch 2/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5470 - acc: 0.7297 - val_loss: 0.5346 - val_acc: 0.7410\n",
      "Epoch 3/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5308 - acc: 0.7366 - val_loss: 0.5380 - val_acc: 0.7359\n",
      "Epoch 4/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5258 - acc: 0.7392 - val_loss: 0.5367 - val_acc: 0.7377\n",
      "Epoch 5/10\n",
      "190/190 [==============================] - 2s 8ms/step - loss: 0.5233 - acc: 0.7413 - val_loss: 0.5339 - val_acc: 0.7424\n",
      "Epoch 6/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5188 - acc: 0.7430 - val_loss: 0.5344 - val_acc: 0.7339\n",
      "Epoch 7/10\n",
      "190/190 [==============================] - 2s 10ms/step - loss: 0.5180 - acc: 0.7437 - val_loss: 0.5423 - val_acc: 0.7339\n",
      "Epoch 8/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5159 - acc: 0.7450 - val_loss: 0.5380 - val_acc: 0.7373\n",
      "Epoch 9/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5154 - acc: 0.7462 - val_loss: 0.5408 - val_acc: 0.7236\n",
      "Epoch 10/10\n",
      "190/190 [==============================] - 2s 9ms/step - loss: 0.5121 - acc: 0.7481 - val_loss: 0.5407 - val_acc: 0.7384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6a5d8df60>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building deep nueral network as a sanity check\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,LeakyReLU,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting and Scaling data\n",
    "X_train,X_other,y_train,y_other = train_test_split(seqs_out,seq_labels_out,train_size=0.7,stratify=seq_labels_out,random_state=22)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_other,y_other,train_size=0.5,stratify=y_other,random_state=22)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Deep Neural Network\n",
    "model = Sequential([\n",
    "    Dense(units=512),\n",
    "    LeakyReLU(),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(units=1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.001),\n",
    "    loss = BinaryCrossentropy(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10,batch_size=256,\n",
    "    validation_data=(X_val,y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.5442 - acc: 0.7294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5442226529121399, 0.7294355630874634]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(\n",
    "    X_test,y_test,batch_size=256\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
